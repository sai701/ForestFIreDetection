{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Frames from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "success = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(\"Video.mp4\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:801: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m success, image \u001b[39m=\u001b[39m vid\u001b[39m.\u001b[39mread()\n\u001b[0;32m      3\u001b[0m names\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m count)\n\u001b[1;32m----> 4\u001b[0m cv2\u001b[39m.\u001b[39;49mimwrite(\u001b[39m\"\u001b[39;49m\u001b[39mframe\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m count, image)\n\u001b[0;32m      5\u001b[0m count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:801: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n"
     ]
    }
   ],
   "source": [
    "while success:\n",
    "    success, image = vid.read()\n",
    "    names.append(\"frame%d.jpg\" % count)\n",
    "    cv2.imwrite(\"frame%d.jpg\" % count, image)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Subtraction Motion Detection in Video & Generating the final output after Masking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is FGMask ?\n",
    "\n",
    "##### FGMask is a computer vision algorithm that uses background subtraction to detect and segment moving objects in an image or video. It is used in many applications such as object tracking, pedestrian detection, and motion segmentation. FGMask is based on the Gaussian Mixture Model (GMM) which uses a weighted sum of Gaussian distributions to model the foreground background image. It is a very efficient algorithm, which can be used for real-time object detection and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# define a video capture object\n",
    "cap = cv2.VideoCapture(\"Video1.mp4\")\n",
    " \n",
    "# variables\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    " \n",
    "while(1):\n",
    "    # read the frames\n",
    "    ret, frame = cap.read()\n",
    "    blur = cv2.GaussianBlur(frame, (1,1), 0)\n",
    "    gray = cv2.cvtColor(blur, cv2.COLOR_BGR2GRAY)\n",
    "    fgmask = fgbg.apply(gray)\n",
    " \n",
    "    # apply threshold\n",
    "    th = cv2.threshold(fgmask.copy(), 127, 255, cv2.THRESH_BINARY)[1]\n",
    "    erode = cv2.erode(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)), iterations=2)\n",
    "    dilated = cv2.dilate(erode, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8,3)), iterations=2)\n",
    "    # find contours\n",
    "    contours, hier = cv2.findContours(erode, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    " \n",
    "    # for each contour, find the bounding rectangle and draw it\n",
    "    for c in contours:\n",
    "        if cv2.contourArea(c) > 1000:\n",
    "            (x,y,w,h) = cv2.boundingRect(c)\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (255,255,0), 2)\n",
    " \n",
    "    # show the frame and the fg masks\n",
    "    # cv2.imshow('frame', frame)\n",
    "    cv2.imshow('fgmask', fgmask)\n",
    " \n",
    "    k = cv2.waitKey(3) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# RGB YCrCb \n",
    "# the main difference is till now they were only using the image and the numerical dataset to predict if there is going to be forest fire or not. But in our method we have ovecme that problem and this method can be implementd in the real world without any problems. It will reduce the manual work to use his brains to see the image identify fire and then take action , whereas in the mdoel developed by us we will reduce the man brain utilizationas he just has to identify the moving pixels, as they are just fire on the basis of intensity.#\n",
    "# Accuracy : This being a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release();\n",
    "cv2.destroyAllWindows();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d8bcec56a5dcd1ee8ebc6caf8a40aee15fe737ebbdbaffd6d3bd5ff977cb14f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
